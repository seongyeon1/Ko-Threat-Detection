{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c299dfca",
   "metadata": {},
   "source": [
    "# DLTHON"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f09308",
   "metadata": {},
   "source": [
    "## DKTC (Dataset of Korean Threatening Conversations)\n",
    "\n",
    "- 텍스트 다중분류 프로젝트"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3216ee81",
   "metadata": {},
   "source": [
    "## 데이터셋 정보"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210ce47f",
   "metadata": {},
   "source": [
    "train.csv\n",
    "\n",
    "    1. idx = 인덱스\n",
    "    2. class = 0~4\n",
    "        class 0; 협박 대화\n",
    "        class 1; 갈취 대화\n",
    "        class 2; 직장 내 괴롭힘 대화\n",
    "        class 3; 기타 괴롭힘 대화\n",
    "    3. conversation = \\n으로 구분된 멀티턴 텍스트 대화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c687c7a",
   "metadata": {},
   "source": [
    "test.json\n",
    "\n",
    "    1. t_### = 인덱스\n",
    "    2. text = 대화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f02370f",
   "metadata": {},
   "source": [
    "submission.csv\n",
    "\n",
    "    1. file_name = 인덱스\n",
    "    2. class = 예측값"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3cd6ba5",
   "metadata": {},
   "source": [
    "## 평가기준 \n",
    "> - 데이터 EDA와 데이터 전처리가 적절하게 이뤄졌는가?\n",
    "> - Task에 알맞게 적절한 모델을 찾아보고 선정했는가?\n",
    "> - 성능향상을 위해 논리적으로 접근했는가?\n",
    "> - 결과 도출을 위해 여러가지 시도를 진행했는가?\n",
    "> - 도출된 결론에 충분한 설득력이 있는가?\n",
    "> - 적절한 metric을 설정하고 그 사용 근거 및 결과를 분석하였는가?\n",
    "> - 발표가 매끄럽게 진행되었고 발표시간을 준수하였는지? (발표 10분-15분)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a04350",
   "metadata": {},
   "source": [
    "## TO-DO-LIST\n",
    "- 일반 대화 데이터셋 만들어야함 (800-1000개정도)\n",
    "- ppt 제작\n",
    "- 평가지표 : f1-score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0486c4f6",
   "metadata": {},
   "source": [
    "## EDA\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "23b9ab76",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T14:30:24.720493Z",
     "start_time": "2024-06-24T14:30:24.717945Z"
    }
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "42350f96",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2024-06-24T14:30:28.504422Z",
     "start_time": "2024-06-24T14:30:28.463144Z"
    }
   },
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "train_data_path =\"./data/train.csv\"\n",
    "train_data = pd.read_csv(train_data_path)\n",
    "train_data.head()"
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "5bdb7636",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T14:30:29.483240Z",
     "start_time": "2024-06-24T14:30:29.473473Z"
    }
   },
   "source": [
    "cate = train_data['class'].unique().tolist()"
   ],
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "b20db170",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T14:30:30.924464Z",
     "start_time": "2024-06-24T14:30:30.920720Z"
    }
   },
   "source": [
    "cate"
   ],
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "da3cfed4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T14:30:33.112914Z",
     "start_time": "2024-06-24T14:30:33.104445Z"
    }
   },
   "source": [
    "intimidation = train_data[train_data['class'] == '협박 대화']\n",
    "extortion = train_data[train_data['class'] == '갈취 대화']\n",
    "harassment_workplace = train_data[train_data['class'] == '직장 내 괴롭힘 대화']\n",
    "harassment_others = train_data[train_data['class'] == '기타 괴롭힘 대화']"
   ],
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "68a1cad2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T14:30:33.682542Z",
     "start_time": "2024-06-24T14:30:33.677460Z"
    }
   },
   "source": [
    "intimidation"
   ],
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "5cda1136",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T14:30:36.960255Z",
     "start_time": "2024-06-24T14:30:36.948177Z"
    }
   },
   "source": [
    "extortion"
   ],
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "9c91b89c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T14:30:37.374819Z",
     "start_time": "2024-06-24T14:30:37.367681Z"
    }
   },
   "source": [
    "harassment_workplace"
   ],
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "be0fe497",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T14:30:38.046530Z",
     "start_time": "2024-06-24T14:30:38.040964Z"
    }
   },
   "source": [
    "harassment_others"
   ],
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "b4e3c0db",
   "metadata": {},
   "source": [
    "일반대화 예시\n",
    "\n",
    "```json\n",
    "{\n",
    "\t\"id\": {\n",
    "\t\t\"text\": \"이거 들어봐 와 이 노래 진짜 좋다 그치 요즘 이 것만 들어 진짜 너무 좋다 내가 요즘 듣는 것도 들어봐 음 난 좀 별론데 좋을 줄 알았는데 아쉽네 내 취향은 아닌 듯 배고프다 밥이나 먹으러 가자 그래\"\n",
    "\t}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "id": "c7c75b06",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T14:31:02.952995Z",
     "start_time": "2024-06-24T14:31:02.908523Z"
    }
   },
   "source": [
    "import json\n",
    "from os.path import join\n",
    "\n",
    "path = join('./data/test.json')\n",
    "        \n",
    "with open(path) as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "test_data = pd.DataFrame(test_data).T\n",
    "test_data.reset_index(drop=True)"
   ],
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "ce92bf62",
   "metadata": {},
   "source": "## Modeling"
  },
  {
   "cell_type": "markdown",
   "id": "adeaa0fe",
   "metadata": {},
   "source": [
    "#### BERT\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": "",
   "id": "7a6c6e5bf00ef070",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "2e489f67",
   "metadata": {},
   "source": [
    "#### train set 과 validation set 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "dfcc27c4",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Stratified Split Train and Validation data \n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=0.2, random_state=777, stratify=train_labels)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "19b65fd8",
   "metadata": {},
   "source": [
    "#### MODEL : klue bert "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eff4a70",
   "metadata": {},
   "source": [
    "#### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "17b0f630",
   "metadata": {},
   "source": [
    "MODEL_PATH = \"klue/bert-base\""
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b433bb01",
   "metadata": {},
   "source": [
    "from transformers import BertTokenizerFast\n",
    "\n",
    "# Load Tokenizer \n",
    "tokenizer = BertTokenizerFast.from_pretrained(MODEL_PATH)\n",
    "\n",
    "# Tokenizing\n",
    "# dict_keys(['input_ids', 'token_type_ids', 'attention_mask']) 이런식으로 \n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True) \n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3a359139",
   "metadata": {},
   "source": [
    "print(dict(val_encodings).keys())"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "89a67e25",
   "metadata": {},
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# trainset-set\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(train_encodings),\n",
    "    train_labels\n",
    "))\n",
    "\n",
    "# validation-set\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(val_encodings),\n",
    "    val_labels\n",
    "))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "9ef493e3",
   "metadata": {},
   "source": [
    "## Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "17e8915e",
   "metadata": {},
   "source": [
    "from transformers import TFBertForSequenceClassification\n",
    "\n",
    "num_labels = len(label_encode)\n",
    "print(num_labels)\n",
    "\n",
    "# TODO : from_pt=False 혹은 없이 해보기\n",
    "# from_pt – (optional) boolean, default False: Load the model weights from a PyTorch state_dict save file (see docstring of pretrained_model_name_or_path argument).\n",
    "model = TFBertForSequenceClassification.from_pretrained(MODEL_PATH, num_labels=num_labels, from_pt=True)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "model.compile(optimizer=optimizer, loss=model.compute_loss, metrics=['accuracy'])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3e33618d",
   "metadata": {},
   "source": [
    "model.compute_loss"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "22ac6484",
   "metadata": {},
   "source": [
    "###  1. callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5f0ff3f4",
   "metadata": {},
   "source": [
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler, ModelCheckpoint\n",
    "\n",
    "callback_earlystopping = EarlyStopping(\n",
    "    monitor=\"val_accuracy\", \n",
    "    min_delta=0.001, # the threshold that triggers the termination (acc should at least improve 0.001)\n",
    "    patience=2)\n",
    "\n",
    "callback_learningrate_scheduler = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.1,\n",
    "    patience=10,\n",
    "    verbose=0,\n",
    "    mode='auto',\n",
    "    min_delta=0.0001,\n",
    "    cooldown=0,\n",
    "    min_lr=0,\n",
    ")\n",
    "\n",
    "\n",
    "callback_modelcheckpoint = ModelCheckpoint(\n",
    "    filepath = \"BERT_BestModel.keras\",\n",
    "    monitor=\"val_accuracy\",\n",
    "    save_best_only=True,\n",
    ")\n",
    "\n",
    "callback_list = [callback_earlystopping, callback_learningrate_scheduler, callback_modelcheckpoint]\n",
    "\n",
    "model.fit(\n",
    "    train_dataset.shuffle(1000).batch(8), epochs=50, batch_size=8,\n",
    "    validation_data=val_dataset.shuffle(1000).batch(16),\n",
    "    callbacks = callback_list\n",
    ")\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "7bb0a93d",
   "metadata": {},
   "source": [
    "#### model, tokenalzer 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "153f3263",
   "metadata": {},
   "source": [
    "MODEL_NAME = 'fine-tuned-klue-bert-base'\n",
    "MODEL_SAVE_PATH = os.path.join(\"/aiffel/aiffel/workplace/20240624/model/\", MODEL_NAME) # change this to your preferred location\n",
    "\n",
    "if os.path.exists(MODEL_SAVE_PATH):\n",
    "    print(f\"{MODEL_SAVE_PATH} -- Folder already exists \\n\")\n",
    "else:\n",
    "    os.makedirs(MODEL_SAVE_PATH, exist_ok=True)\n",
    "    print(f\"{MODEL_SAVE_PATH} -- Folder create complete \\n\")\n",
    "\n",
    "# save tokenizer, model\n",
    "model.save_pretrained(MODEL_SAVE_PATH)\n",
    "tokenizer.save_pretrained(MODEL_SAVE_PATH)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b60c5434",
   "metadata": {},
   "source": [
    "from transformers import TextClassificationPipeline\n",
    "\n",
    "# Load Fine-tuning model\n",
    "loaded_tokenizer = BertTokenizerFast.from_pretrained(MODEL_SAVE_PATH)\n",
    "loaded_model = TFBertForSequenceClassification.from_pretrained(MODEL_SAVE_PATH)\n",
    "\n",
    "text_classifier = TextClassificationPipeline(\n",
    "    tokenizer=loaded_tokenizer, \n",
    "    model=loaded_model, \n",
    "    framework='tf',\n",
    "    return_all_scores=True\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4365da15",
   "metadata": {},
   "source": [
    "test_df = pd.DataFrame(['file_name', 'class'])\n",
    "test_df.head()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "5a0aa72d",
   "metadata": {},
   "source": [
    "### inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7cb0bbe5",
   "metadata": {},
   "source": [
    "import json\n",
    "\n",
    "path = join('/aiffel/aiffel/dktc/data/test.json')\n",
    "        \n",
    "with open(path) as f:\n",
    "    test_data = json.load(f)\n",
    "    \n",
    "# test_json\n",
    "\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "4b77dbcf",
   "metadata": {},
   "source": [
    "### predict : tqdm 진행률(progress) 표시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "99a28a30",
   "metadata": {},
   "source": [
    "from tqdm.auto import tqdm\n",
    "answer_dict = {}\n",
    "for file_name, text in tqdm(test_data.items()):\n",
    "    preds_list = text_classifier(text['text'])[0]\n",
    "    best_label = int(sorted(preds_list, key=lambda x : x['score'])[-1]['label'].split('_')[-1])\n",
    "    answer_dict[file_name] = best_label\n",
    "          \n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "91316137",
   "metadata": {},
   "source": [
    "answer_dict"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e273dc5c",
   "metadata": {},
   "source": [
    "for key, value in answer_dict.items():\n",
    "    test_df = test_df.append({'file_name': key, 'class': value}, ignore_index=True)\n",
    "\n",
    "test_df\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7cc6524b",
   "metadata": {},
   "source": [
    "\n",
    "## 저장\n",
    "test_df = test_df[2:]\n",
    "test_df = test_df[['class', 'file_name']]\n",
    "test_df['class'] = test_df['class'].astype('int32')\n",
    "test_df.set_index('file_name', inplace=True)\n",
    "\n",
    "test_df.to_csv('FIN_BERT_L_0598.csv', index=\"file_name\")\n",
    "\n",
    "\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "305c166a",
   "metadata": {},
   "source": [
    "test_df"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
