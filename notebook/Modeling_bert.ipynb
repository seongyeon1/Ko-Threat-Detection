{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e608c83",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "id": "aa841b41",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T13:17:08.557461Z",
     "start_time": "2024-06-25T13:17:08.553511Z"
    }
   },
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "3835cf57",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T13:17:08.610166Z",
     "start_time": "2024-06-25T13:17:08.559557Z"
    }
   },
   "source": [
    "train_df_path = \"./data/train.csv\"\n",
    "train_df = pd.read_csv(train_df_path)\n",
    "train_df.head()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   idx      class                                       conversation\n",
       "0    0      협박 대화  지금 너 스스로를 죽여달라고 애원하는 것인가?\\n 아닙니다. 죄송합니다.\\n 죽을 ...\n",
       "1    1      협박 대화  길동경찰서입니다.\\n9시 40분 마트에 폭발물을 설치할거다.\\n네?\\n똑바로 들어 ...\n",
       "2    2  기타 괴롭힘 대화  너 되게 귀여운거 알지? 나보다 작은 남자는 첨봤어.\\n그만해. 니들 놀리는거 재미...\n",
       "3    3      갈취 대화  어이 거기\\n예??\\n너 말이야 너. 이리 오라고\\n무슨 일.\\n너 옷 좋아보인다?...\n",
       "4    4      갈취 대화  저기요 혹시 날이 너무 뜨겁잖아요? 저희 회사에서 이 선크림 파는데 한 번 손등에 ..."
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>class</th>\n",
       "      <th>conversation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>협박 대화</td>\n",
       "      <td>지금 너 스스로를 죽여달라고 애원하는 것인가?\\n 아닙니다. 죄송합니다.\\n 죽을 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>협박 대화</td>\n",
       "      <td>길동경찰서입니다.\\n9시 40분 마트에 폭발물을 설치할거다.\\n네?\\n똑바로 들어 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>기타 괴롭힘 대화</td>\n",
       "      <td>너 되게 귀여운거 알지? 나보다 작은 남자는 첨봤어.\\n그만해. 니들 놀리는거 재미...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>갈취 대화</td>\n",
       "      <td>어이 거기\\n예??\\n너 말이야 너. 이리 오라고\\n무슨 일.\\n너 옷 좋아보인다?...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>갈취 대화</td>\n",
       "      <td>저기요 혹시 날이 너무 뜨겁잖아요? 저희 회사에서 이 선크림 파는데 한 번 손등에 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "866d4547",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T13:17:08.621945Z",
     "start_time": "2024-06-25T13:17:08.612235Z"
    }
   },
   "source": [
    "train_df.groupby(by=['class']).count()\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "              idx  conversation\n",
       "class                          \n",
       "갈취 대화         981           981\n",
       "기타 괴롭힘 대화    1094          1094\n",
       "직장 내 괴롭힘 대화   979           979\n",
       "협박 대화         896           896"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>conversation</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>갈취 대화</th>\n",
       "      <td>981</td>\n",
       "      <td>981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>기타 괴롭힘 대화</th>\n",
       "      <td>1094</td>\n",
       "      <td>1094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>직장 내 괴롭힘 대화</th>\n",
       "      <td>979</td>\n",
       "      <td>979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>협박 대화</th>\n",
       "      <td>896</td>\n",
       "      <td>896</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "95b6e73e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T13:17:08.634727Z",
     "start_time": "2024-06-25T13:17:08.628031Z"
    }
   },
   "source": [
    "label_encode = {\n",
    "    \"협박 대화\" : 0,\n",
    "    \"갈취 대화\" : 1,\n",
    "    \"직장 내 괴롭힘 대화\" : 2,\n",
    "    \"기타 괴롭힘 대화\" : 3,   \n",
    "}\n",
    "train_df['encoded_label'] = train_df['class'].map(label_encode)"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "2e42e9d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T13:17:08.650935Z",
     "start_time": "2024-06-25T13:17:08.645018Z"
    }
   },
   "source": [
    "train_texts = train_df['conversation'].to_list()\n",
    "train_labels = train_df['encoded_label'].to_list()"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "79def0aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T13:17:09.842748Z",
     "start_time": "2024-06-25T13:17:09.168817Z"
    }
   },
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Stratified Split Train and Validation data \n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=0.2, random_state=1004, stratify=train_labels)"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "6ef40920",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T13:17:09.851642Z",
     "start_time": "2024-06-25T13:17:09.847999Z"
    }
   },
   "source": [
    "MODEL_PATH = \"klue/bert-base\""
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "babab1fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T13:17:15.649407Z",
     "start_time": "2024-06-25T13:17:10.388105Z"
    }
   },
   "source": [
    "from transformers import BertTokenizerFast\n",
    "\n",
    "# Load Tokenizer \n",
    "tokenizer = BertTokenizerFast.from_pretrained(MODEL_PATH)\n",
    "\n",
    "# Tokenizing\n",
    "# dict_keys(['input_ids', 'token_type_ids', 'attention_mask']) 이런식으로 \n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True) \n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/289 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2093cdd2351b4dc1b051a2b849f4d57b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/248k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "746408e8416641b08e3ceb806567a4e6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/495k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5875da8990cd43dfa5170e6c67f52afc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fea7626ab9d7481ab62f9d06cb3d689a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/DL-tf/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/425 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "500b90972ea9454b8370bef70e14d482"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "f9004858",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T13:17:15.656324Z",
     "start_time": "2024-06-25T13:17:15.651783Z"
    }
   },
   "source": [
    "print(dict(val_encodings).keys())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "39f247d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T13:17:25.808180Z",
     "start_time": "2024-06-25T13:17:15.658006Z"
    }
   },
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# trainset-set\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(train_encodings),\n",
    "    train_labels\n",
    "))\n",
    "\n",
    "# validation-set\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(val_encodings),\n",
    "    val_labels\n",
    "))"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "id": "bb2f4744",
   "metadata": {},
   "source": [
    "## FineTunnig\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "43997003",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T13:17:37.985787Z",
     "start_time": "2024-06-25T13:17:25.811671Z"
    }
   },
   "source": [
    "from transformers import TFBertForSequenceClassification\n",
    "\n",
    "num_labels = len(label_encode)\n",
    "print(num_labels)\n",
    "# TODO : from_pt=False 혹은 없이 해보기\n",
    "# from_pt – (optional) boolean, default False: Load the model weights from a PyTorch state_dict save file (see docstring of pretrained_model_name_or_path argument).\n",
    "model = TFBertForSequenceClassification.from_pretrained(MODEL_PATH, num_labels=num_labels, from_pt=True)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "model.compile(optimizer=optimizer, loss=model.compute_loss, metrics=['accuracy'])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/445M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5d2e799f6ea942f199872018315a5805"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertForSequenceClassification: ['bert.embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "3b125e6d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T13:17:37.993604Z",
     "start_time": "2024-06-25T13:17:37.988067Z"
    }
   },
   "source": [
    "model.compute_loss"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method TFPreTrainedModel.compute_loss of <transformers.models.bert.modeling_tf_bert.TFBertForSequenceClassification object at 0x31d3fcf10>>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "id": "f8f95b30",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T13:22:04.719427Z",
     "start_time": "2024-06-25T13:22:04.421366Z"
    }
   },
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler, ModelCheckpoint\n",
    "\n",
    "callback_earlystopping = EarlyStopping(\n",
    "    monitor=\"val_accuracy\", \n",
    "    min_delta=0.001, # the threshold that triggers the termination (acc should at least improve 0.001)\n",
    "    patience=2)\n",
    "\n",
    "callback_learningrate_scheduler = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.1,\n",
    "    patience=10,\n",
    "    verbose=0,\n",
    "    mode='auto',\n",
    "    min_delta=0.0001,\n",
    "    cooldown=0,\n",
    "    min_lr=0,\n",
    ")\n",
    "\n",
    "\n",
    "callback_modelcheckpoint = ModelCheckpoint(\n",
    "    filepath = \"BERT_BestModel.keras\",\n",
    "    monitor=\"val_accuracy\",\n",
    "    save_best_only=True,\n",
    ")\n",
    "\n",
    "callback_list = [callback_earlystopping, callback_learningrate_scheduler, callback_modelcheckpoint]\n",
    "\n",
    "model.fit(\n",
    "    train_dataset.shuffle(1000).batch(8), epochs=50, batch_size=8,\n",
    "    validation_data=val_dataset.shuffle(1000).batch(16),\n",
    "    callbacks = callback_list\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "in user code:\n\n    File \"/opt/homebrew/Caskroom/miniforge/base/envs/DL-tf/lib/python3.8/site-packages/keras/engine/training.py\", line 1051, in train_function  *\n        return step_function(self, iterator)\n    File \"/opt/homebrew/Caskroom/miniforge/base/envs/DL-tf/lib/python3.8/site-packages/keras/engine/training.py\", line 1040, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/homebrew/Caskroom/miniforge/base/envs/DL-tf/lib/python3.8/site-packages/keras/engine/training.py\", line 1030, in run_step  **\n        outputs = model.train_step(data)\n    File \"/opt/homebrew/Caskroom/miniforge/base/envs/DL-tf/lib/python3.8/site-packages/transformers/modeling_tf_utils.py\", line 1630, in train_step\n        x, y, sample_weight = keras.utils.unpack_x_y_sample_weight(data)\n\n    AttributeError: module 'keras.utils' has no attribute 'unpack_x_y_sample_weight'\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[15], line 28\u001B[0m\n\u001B[1;32m     20\u001B[0m callback_modelcheckpoint \u001B[38;5;241m=\u001B[39m ModelCheckpoint(\n\u001B[1;32m     21\u001B[0m     filepath \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBERT_BestModel.keras\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     22\u001B[0m     monitor\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mval_accuracy\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     23\u001B[0m     save_best_only\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m     24\u001B[0m )\n\u001B[1;32m     26\u001B[0m callback_list \u001B[38;5;241m=\u001B[39m [callback_earlystopping, callback_learningrate_scheduler, callback_modelcheckpoint]\n\u001B[0;32m---> 28\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     29\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrain_dataset\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshuffle\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1000\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbatch\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m8\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m50\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m8\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     30\u001B[0m \u001B[43m    \u001B[49m\u001B[43mvalidation_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mval_dataset\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshuffle\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1000\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbatch\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m16\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     31\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mcallback_list\u001B[49m\n\u001B[1;32m     32\u001B[0m \u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/DL-tf/lib/python3.8/site-packages/transformers/modeling_tf_utils.py:1229\u001B[0m, in \u001B[0;36mTFPreTrainedModel.fit\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1226\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(keras\u001B[38;5;241m.\u001B[39mModel\u001B[38;5;241m.\u001B[39mfit)\n\u001B[1;32m   1227\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfit\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m   1228\u001B[0m     args, kwargs \u001B[38;5;241m=\u001B[39m convert_batch_encoding(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m-> 1229\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/DL-tf/lib/python3.8/site-packages/keras/utils/traceback_utils.py:67\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     65\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:  \u001B[38;5;66;03m# pylint: disable=broad-except\u001B[39;00m\n\u001B[1;32m     66\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[0;32m---> 67\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     68\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m     69\u001B[0m   \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[0;32m/var/folders/1b/dx8_94fs0q740wx3lfknk6y40000gn/T/__autograph_generated_filezi5xqaxo.py:15\u001B[0m, in \u001B[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001B[0;34m(iterator)\u001B[0m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m     14\u001B[0m     do_return \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m---> 15\u001B[0m     retval_ \u001B[38;5;241m=\u001B[39m ag__\u001B[38;5;241m.\u001B[39mconverted_call(ag__\u001B[38;5;241m.\u001B[39mld(step_function), (ag__\u001B[38;5;241m.\u001B[39mld(\u001B[38;5;28mself\u001B[39m), ag__\u001B[38;5;241m.\u001B[39mld(iterator)), \u001B[38;5;28;01mNone\u001B[39;00m, fscope)\n\u001B[1;32m     16\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m:\n\u001B[1;32m     17\u001B[0m     do_return \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/DL-tf/lib/python3.8/site-packages/transformers/modeling_tf_utils.py:1630\u001B[0m, in \u001B[0;36mTFPreTrainedModel.train_step\u001B[0;34m(self, data)\u001B[0m\n\u001B[1;32m   1627\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_using_dummy_loss \u001B[38;5;129;01mand\u001B[39;00m parse(tf\u001B[38;5;241m.\u001B[39m__version__) \u001B[38;5;241m<\u001B[39m parse(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m2.11.0\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m   1628\u001B[0m     \u001B[38;5;66;03m# Newer TF train steps leave this out\u001B[39;00m\n\u001B[1;32m   1629\u001B[0m     data \u001B[38;5;241m=\u001B[39m expand_1d(data)\n\u001B[0;32m-> 1630\u001B[0m x, y, sample_weight \u001B[38;5;241m=\u001B[39m \u001B[43mkeras\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mutils\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43munpack_x_y_sample_weight\u001B[49m(data)\n\u001B[1;32m   1631\u001B[0m \u001B[38;5;66;03m# If the inputs are mutable dictionaries, make a shallow copy of them because we will modify\u001B[39;00m\n\u001B[1;32m   1632\u001B[0m \u001B[38;5;66;03m# them during input/label pre-processing. This avoids surprising the user by wrecking their data.\u001B[39;00m\n\u001B[1;32m   1633\u001B[0m \u001B[38;5;66;03m# In addition, modifying mutable Python inputs makes XLA compilation impossible.\u001B[39;00m\n\u001B[1;32m   1634\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(x, \u001B[38;5;28mdict\u001B[39m):\n",
      "\u001B[0;31mAttributeError\u001B[0m: in user code:\n\n    File \"/opt/homebrew/Caskroom/miniforge/base/envs/DL-tf/lib/python3.8/site-packages/keras/engine/training.py\", line 1051, in train_function  *\n        return step_function(self, iterator)\n    File \"/opt/homebrew/Caskroom/miniforge/base/envs/DL-tf/lib/python3.8/site-packages/keras/engine/training.py\", line 1040, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/homebrew/Caskroom/miniforge/base/envs/DL-tf/lib/python3.8/site-packages/keras/engine/training.py\", line 1030, in run_step  **\n        outputs = model.train_step(data)\n    File \"/opt/homebrew/Caskroom/miniforge/base/envs/DL-tf/lib/python3.8/site-packages/transformers/modeling_tf_utils.py\", line 1630, in train_step\n        x, y, sample_weight = keras.utils.unpack_x_y_sample_weight(data)\n\n    AttributeError: module 'keras.utils' has no attribute 'unpack_x_y_sample_weight'\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "id": "aa326acd",
   "metadata": {},
   "source": [
    "## 모델, 토크나이저 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ae5c50ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_model/fine-tuned-klue-bert-base -- Folder create complete \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('_model/fine-tuned-klue-bert-base/tokenizer_config.json',\n",
       " '_model/fine-tuned-klue-bert-base/special_tokens_map.json',\n",
       " '_model/fine-tuned-klue-bert-base/vocab.txt',\n",
       " '_model/fine-tuned-klue-bert-base/added_tokens.json',\n",
       " '_model/fine-tuned-klue-bert-base/tokenizer.json')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_NAME = 'fine-tuned-klue-bert-base'\n",
    "MODEL_SAVE_PATH = os.path.join(\"_model\", MODEL_NAME) # change this to your preferred location\n",
    "\n",
    "if os.path.exists(MODEL_SAVE_PATH):\n",
    "    print(f\"{MODEL_SAVE_PATH} -- Folder already exists \\n\")\n",
    "else:\n",
    "    os.makedirs(MODEL_SAVE_PATH, exist_ok=True)\n",
    "    print(f\"{MODEL_SAVE_PATH} -- Folder create complete \\n\")\n",
    "\n",
    "# save tokenizer, model\n",
    "model.save_pretrained(MODEL_SAVE_PATH)\n",
    "tokenizer.save_pretrained(MODEL_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bf8f2ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at _model/fine-tuned-klue-bert-base were not used when initializing TFBertForSequenceClassification: ['dropout_113']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at _model/fine-tuned-klue-bert-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TextClassificationPipeline\n",
    "\n",
    "# Load Fine-tuning model\n",
    "loaded_tokenizer = BertTokenizerFast.from_pretrained(MODEL_SAVE_PATH)\n",
    "loaded_model = TFBertForSequenceClassification.from_pretrained(MODEL_SAVE_PATH)\n",
    "\n",
    "text_classifier = TextClassificationPipeline(\n",
    "    tokenizer=loaded_tokenizer, \n",
    "    model=loaded_model, \n",
    "    framework='tf',\n",
    "    return_all_scores=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "85ed7512",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>file_name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>class</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0\n",
       "0  file_name\n",
       "1      class"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.DataFrame(['file_name', 'class'])\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "91735df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('./data/test.json', 'r') as f:\n",
    "    test_json = json.load(f)\n",
    "    \n",
    "# test_json"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
